<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta class="viewport" content="width=device-width, initial-scale=1.0">	
	<title>Unidad 4</title>
	<link rel="stylesheet" href="U4.css">
</head>
<body>
	<nav class="navegacion">
		<div id="its">
			<img src="D:imagenes/ITS.png"= alt = "">
		</div>
		<ul>
			<li><a href="index.html">Inicio</a></li>
			<li><a href="U1.html">Unidad 1</a></li>
			<li><a href="U2.html">Unidad 2</a></li>
			<li><a href="U3.html">Unidad 3</a></li>
			<li><a href="U4.html">Unidad 4</a></li>
		</ul>
	</nav>
	<div class="main">
		<section id="Temario">
			<div id="temas">
				<h1>
					<b>Temario Unidad 4</b>
					<br>
					Procesamiento Paralelo
					<br>
					<br><b>4.1 Aspectos Básicos de la computación paralela.</b>
					<br>
					<br><b>4.2 Tipos de computación paralela.</b>
					<br>
					<br><i>4.2.1 Clasificación.</i>
					<br>
					<br><i>4.2.2 Arquitectura de computadoras secuenciales.</i>
					<br>
					<br><i>4.2.3 Organización de direcciones de memoria.</i>
					<br>
					<br><b>4.3 Sistemas de memoria (compartida).</b>
					<br>
					<br><i>4.3.1 Redes de interconexión dinámica (indirecta).</i>
					<br>
					<br><i>4.3.2 Medio compartido.</i>
					<br>
					<br><i>4.3.3 Conmutadas.</i>
					<br>
					<br><b>4.4 Sistemas de memoria distribuida.</b>
					<br>
					<br><i>4.4.1 Multicomputadores.</i>
					<br>
					<br><i>4.4.2 Redes de interconexión estáticas.</i>
					<br>
					<br><b>4.5 Casos para estudio.</b>

				</h1>
			</div>
		</section>
	</div>
	<div class="arco">
		<section id="AB">
			<div id="ab">
				<h1>
					<b>Aspectos básicos de la computación paralela.</b><br>
					La computación paralela es una forma de cómputo en la que muchas instrucciones se ejecutan simultáneamente, operando sobre el principio de que problemas grandes, a menudo se pueden dividir en unos más pequeños, que luego son resueltos simultáneamente (en paralelo). Hay varias formas diferentes de computación paralela: paralelismo a nivel de bit, paralelismo a nivel de instrucción, paralelismo de datos y paralelismo de tareas. El paralelismo se ha empleado durante muchos años, sobre todo en la computación de altas prestaciones, pero el interés en ella ha crecido últimamente debido a las limitaciones físicas que impiden el aumento de la frecuencia. Como el consumo de energía —y por consiguiente la generación de calor— de las computadoras constituye una preocupación en los últimos años, la computación en paralelo se ha convertido en el paradigma dominante en la arquitectura de computadores, principalmente en forma de procesadores multinúcleo.
				</h1>
			</div>
		</section>
		<section id="TCP">
			<div id="tcp">
				<h1>
					<b>Tipos de computación paralela</b><br>
					<ul>
						<li><i>Paralelismo a nivel bit:</i> Se habla de paralelismo al nivel de bit, cuando se aumenta el tamaño de la palabra del procesador (tamaño de la cadena de bits a procesar). Este aumento reduce el número de instrucciones que tiene que ejecutar el procesador en variables cuyos tamaños sean mayores a la longitud de la cadena.</li>
						<br>
						<li><i>Paralelismo a nivel de instrucción:</i> El paralelismo a nivel de instrucción  consiste  en un técnica que busca que la combinación de instrucciones de bajo nivel que ejecuta un procesador puedan ser ordenadas de forma tal que al ser procesadas en simultáneo no afecten el resultado final del programa, y más bien incrementen la velocidad y aprovechen al máximo las capacidades del hardware. Un pipeline (canalizador) de instrucciones es el que permite que por cada ciclo de reloj  del procesador múltiples instrucciones se encuentren en distintas fases de ejecución. </li>
						<br>
						<li><i>Paralelismo de datos:</i> El paralelismo de datos es el paralelismo inherente en programas con ciclos, que se centra en la distribución de los datos entre los diferentes nodos computacionales que deben tratarse en paralelo. La paralelización de ciclos conduce a menudo a secuencias similares de operaciones —no necesariamente idénticas— o funciones que se realizan en los elementos de una gran estructura de datos. Muchas de las aplicaciones científicas y de ingeniería muestran paralelismo de datos.</li>
						<br>
						<li><i>Paralelismo de tareas:</i> El paralelismo de tareas es la característica de un programa paralelo en la que cálculos completamente diferentes se pueden realizar en cualquier conjunto igual o diferente de datos. Esto contrasta con el paralelismo de datos, donde se realiza el mismo cálculo en distintos o mismos grupos de datos. El paralelismo de tareas por lo general no escala con el tamaño de un problema.</li>
						<br>
					</ul>
				</h1>
			</div>
		</section>
		<section id="CL">
			<div id="cl">
				<h1>
					<b>Clasificación.</b><br>
					<br>
				<ul>
					<li><i>Computación multinúcleo:</i> Un procesador multinúcleo es un procesador que incluye múltiples unidades de ejecución (núcleos) en el mismo chip. Un procesador multinúcleo puede ejecutar múltiples instrucciones por ciclo de secuencias de instrucciones múltiples.</li><br>
					<li><i>Multiprocesamiento simétrico:</i> Un multiprocesador simétrico (SMP) es un sistema computacional con múltiples procesadores idénticos que comparten memoria y se conectan a través de un bus. La contención del bus previene el escalado de esta arquitectura.</li><br>
					<li><i>Computación en clúster:</i> un clúster es un grupo de ordenadores débilmente acoplados que trabajan en estrecha colaboración, de modo que en algunos aspectos pueden considerarse como un solo equipo.</li><br>
					<li><i>Procesamiento paralelo masivo:</i> tienden a ser más grandes que los clústeres, con «mucho más» de 100 procesadores. En un MPP, cada CPU tiene su propia memoria y una copia del sistema operativo y la aplicación.</li><br>
					<li><i>Computación distribuida:</i> la computación distribuida es la forma más distribuida de la computación paralela. Se hace uso de ordenadores que se comunican a través de la Internet para trabajar en un problema dado.</li><br>
					<li><i>Computadoras paralelas especializadas:</i> dentro de la computación paralela, existen dispositivos paralelos especializados que generan interés. Aunque no son específicos para un dominio, tienden a ser aplicables sólo a unas pocas clases de problemas paralelos.</li><br>
					<li><i>Cómputo reconfigurable con arreglos de compuertas programables:</i> el cómputo reconfigurable es el uso de un arreglo de compuertas programables (FPGA) como coprocesador de un ordenador de propósito general.</li><br>
					<li><i>Cómputo de propósito general en unidades de procesamiento gráfico (GPGPU):</i> es una tendencia relativamente reciente en la investigación de ingeniería informática. Los GPUs son co-procesadores que han sido fuertemente optimizados para procesamiento de gráficos por computadora.</li><br>
					<li><i>Circuitos integrados de aplicación específica:</i> debido a que un ASIC (por definición) es específico para una aplicación dada, puede ser completamente optimizado para esa aplicación. Como resultado, para una aplicación dada, un ASIC tiende a superar a un ordenador de propósito general.</li><br>
					<li><i>Procesadores vectoriales:</i> pueden ejecutar la misma instrucción en grandes conjuntos de datos. Tienen operaciones de alto nivel que trabajan sobre arreglos lineales de números o vectores.</li><br>
				</ul>					
				</h1>
			</div>
		</section>
		<section id="ACS">
			<div id="acs">
				<h1>
					<b>Arquitectura de computadores secuenciales.</b>
					<br>
					En los sistemas secuenciales, los valores de las salidas, en un momento dado, no dependen exclusivamente de los valores de las entradas en dicho momento, sino también dependen del estado anterior o estado interno. El sistema secuencial más simple es el biestable, de los cuales, el de tipo D (o cerrojo) es el más utilizado actualmente. El sistema secuencial requiere de la utilización de un dispositivo de memoria que pueda almacenar la historia pasada de sus entradas (denominadas variables de estado) y le permita mantener su estado durante algún tiempo, estos dispositivos de memoria pueden ser sencillos como un simple retardador o celdas de memoria de tipo DRAM, SRAM o multivibradores biestables también conocido como Flip-Flop.<br>
					Dentro de esta clasificación, se encuentra a las computadoras SISD (Single-Instruction Stream, Single-Data, Stream/ Flujo único de instrucciones, flujo único de datos).<br>
					Las computadoras secuenciales se basan en el modelo introducido por por John Von Neumann la cual consiste en:
					<ul>
						<br>
						<li>Una Unidad Central de Procesamiento (CPU)</li>
						<li>Memoria principal para almacenar información.</li>
						<li>Bus donde fluyan los datos.</li>
						<li>Mecanismo de sincronización.</li>
					</ul>
				</h1>
			</div>
		</section>
		<section id="ODM">
			<div id="odm">
				<h1>
					<b>Organización de direcciones de memoria.</b>
					<br>
					La memoria principal en un ordenador en paralelo puede ser compartida —compartida entre todos los elementos de procesamiento en un único espacio de direcciones—, o distribuida —cada elemento de procesamiento tiene su propio espacio local de direcciones—. El término memoria distribuida se refiere al hecho de que la memoria se distribuye lógicamente, pero a menudo implica que también se distribuyen físicamente. La memoria distribuida-compartida y la virtualización de memoria combinan los dos enfoques, donde el procesador tiene su propia memoria local y permite acceso a la memoria de los procesadores que no son locales. Los accesos a la memoria local suelen ser más rápidos que los accesos a memoria no local.<br>
					Las arquitecturas de ordenador en las que cada elemento de la memoria principal se puede acceder con igual latencia y ancho de banda son conocidas como arquitecturas de acceso uniforme a memoria (UMA). Típicamente, sólo se puede lograr con un sistema de memoria compartida, donde la memoria no está distribuida físicamente. Un sistema que no tiene esta propiedad se conoce como arquitectura de acceso a memoria no uniforme (NUMA). Los sistemas de memoria distribuidos tienen acceso no uniforme a la memoria.
				</h1>
			</div>
		</section>
		<section id="SMC">
			<div id="smc">
				<h1>
					<b>Sistemas de memoria compartida (multiprocesadores).</b>
					<br>
					La mayoría de los multiprocesadores comerciales son del tipo UMA (Uniform Memory Access): todos los procesadores tienen igual tiempo de acceso a la memoria compartida. En la arquitectura UMA los procesadores se conectan a la memoria a través de un bus, una red multietapa o un conmutador de barras cruzadas (red multietapa o un conmutador de barras cruzadas (crossbar crossbar) y disponen de su propia ) y disponen de su propia memoria caché. Los procesadores tipo NUMA (Non Uniform Memory Access) presentan tiempos de acceso a la memoria compartida que dependen de la ubicación del elemento de proceso y la memoria.
				</h1>
			</div>
		</section>
		<section id="RID">
			<div id="rid">
				<h1>
					<b>Redes de interconexión dinámica (indirecta).</b>
					<br>
					<b>Medio compartido.</b>
					<b>Conexión por bus compartido.</b>
					<br>
					Una red dinámica es una red cuya topología puede variar durante el curso de la ejecución de un programa paralelo o entre dos ejecuciones de programas. La red está constituida  por elementos materiales específicos, llamados commutadores o switches. Las redes dinámicas se utilizan  sobre todo en los multiprocesadores. En este caso, la red une  los procesadores a los bancos de memoria central. Cualquier acceso de un procesador a la memoria (bien sea para acceder a los datos o a las instrucciones) debe pasar a través de la red, por lo se dice que la red tiene un  acoplamiento fuerte. La red debe poseer un rendimiento extremadamente bueno para no demorar demasiado a los procesadores que acceden a memoria.
				</h1>
			</div>
		</section>
		<section id="SMD">
			<div id="smd">
				<h1>
					<b>Sistemas de memoria distribuida (multicomputadores).</b><br>
					<br>
					Los sistemas de memoria distribuida o multicomputadores pueden ser de dos tipos básicos. El primer de ellos consta de un único computador con múltiples CPUs comunicadas por un bus de datos mientras que en el segundo se utilizan múltiples computadores, cada uno con su propio procesador, enlazados por una red de interconexión más o menos rápida.<br>
					Sobre los sistemas de multicomputadores de memoria distribuida, se simula memorias compartidas. Se usan los mecanismos de comunicación y sincronización de sistemas multiprocesadores.<br>
					En esta arquitectura, el computador paralelo es esencialmente una colección de procesadores secuenciales, cada uno con su propia memoria local, que pueden trabajar conjuntamente.
					<ul>
						<li>Cada nodo tiene rápido acceso a su propia memoria y acceso a la memoria de otros nodos mediante una red de comunicaciones, habitualmente una red de comunicaciones de alta velocidad.</li>
						<li>Los datos son intercambiados entre los nodos como mensajes a través de la red.</li>
						<li>Una red de ordenadores, especialmente si disponen de una interconexión de alta velocidad, puede ser vista como un multicomputador de memoria distribuida y como tal ser utilizada para resolver problemas mediante computación paralela.</li>
					</ul>
				</h1>
			</div>
		</section>
		<section id="RIE">
			<div id="rie">
				<h1>
					<b>Redes de interconexión estáticas.</b>
					Las redes estáticas se usan habitualmente en los multicomputadores. Aunque hay muchas topologías  posibles, los multicomputadores comerciales habitualmente usan sólo dos o tres configuraciones. En los multicomputadores de la primera generación la topología preferida era el hipercubo, mientras que en  la actualidad, y gracias a usar un encaminamiento de mensajes segmentado, se utiliza con más frecuencia la malla o el toro.
				</h1>
			</div>
		</section>
		<section id="CE">
			<div id="ce">
				<h1>
					<b>Casos para estudio</b><br>
					Descripción:
					<br>
					La programación paralela involucra muchos aspectos que no se presenta en la programación convencional (secuencial). El diseño de un programa paralelo tiene que considerar entre otras cosas, el tipo de arquitectura sobre la cual se va a ejecutar el programa, las necesidades de tiempo y espacio que requiere la aplicación, el modelo de programación paralelo adecuado para implantar la aplicación y la forma de coordinar y comunicar a diferentes procesadores para que resuelvan un problema común. Existen varias herramientas disponibles para programación paralela. En el curso utilizaremos PVM y MPI, dado su alta disponibilidad para computadoras diferentes y su aceptación en la comunidad académica.
					<br>
					<br><b>Procesamiento en paralelo</b>
					<br>
					Es un proceso empleado para acelerar el tiempo de ejecución de un programa dividiéndolo en múltiples trozos que se ejecutarán al mismo tiempo, cada uno en su propios procesadores.
					<br>
					La tecnología detrás del desarrollo de componentes de sistemas computacionales ha alcanzado su madurez y los desarrollos están a punto de producirse en la era del procesamiento en paralelo, lo que significa que la tecnología de la computación paralela necesita avanzar, aún cuando no está lo suficientemente madura como para ser explotado como una tecnología de disponibilidad masiva. La razón de ser del procesamiento en paralelo es acelerar la resolución de un problema, la aceleración que puede alcanzarse depende tanto del problema en sí como de la arquitectura de la computadora. el hardware de la máquina entra en juego ya que es preciso maximizar la relación entre el tiempo de cálculo útil y el perdido en el paso de mensajes, parámetros que dependen de la capacidad de proceso de las CPUs y de la velocidad de la red de comunicaciones.
					<br>
					Si las condiciones son muy favorables es incluso posible alcanzar la aceleración superlineal, consistente en que el programa se ejecuta aún más rápido que en régimen linea, la aparente paradoja se da debido a que cada procesador cuenta con su propia memoria ordinaria y caché, que pueden ser usadas de forma más eficiente con un subconjunto de datos, de hecho, es posible que el problema no se pueda resolver en un único procesador pero sí sobre un conjunto de ordenadores debidamente configurados, simplemente por cuestión de tamaño de los datos.
				</h1>
			</div>
		</section>
	</div>
</body>
</html>